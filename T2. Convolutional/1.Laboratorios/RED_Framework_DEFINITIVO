{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**FRAMEWORK DEEP LEARNING**\n"],"metadata":{"id":"a6765jQWL9li"}},{"cell_type":"markdown","source":["# **InteractiveShell**"],"metadata":{"id":"gFS6J6h53YEZ"}},{"cell_type":"code","source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","from platform import python_version\n","'Python ' + python_version()\n","import numpy as np\n","import pandas as pd\n","import sys\n","import math\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from scipy.signal import convolve2d"],"metadata":{"id":"S66jUQGbCWmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **RED**\n"],"metadata":{"id":"Tq63T8DmMoa0"}},{"cell_type":"markdown","source":["# **Convolucional**\n"],"metadata":{"id":"SY1SIB6EqwVJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRaGtjn-L5E2"},"outputs":[],"source":["class MyConv:\n","    def __init__(self,**kwargs):\n","        self.c_filter = kwargs.get('c_filter', 0)\n","        self.c_pad = kwargs.get('c_pad', 0)\n","        self.c_stride = kwargs.get('c_stride', 0)\n","        self.c_filters = kwargs.get('c_filters', 0)\n","        self.activation = kwargs.get('activation', 'relu')\n","        self.p_stride = kwargs.get('p_stride', 0)\n","        self.p_filter = kwargs.get('p_filter', 0)\n","        self.p_function = kwargs.get('p_function', 'max')\n","        self.c_channels = kwargs.get('c_channels', 0)\n","        self.W = np.random.randn(self.c_filter, self.c_filter, self.c_channels, self.c_filters)\n","        self.b = np.random.randn(1, 1, 1, 8)\n","\n","    def add_pad(self,data_in,pad):\n","      return np.pad(data_in, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant')\n","\n","    def one_convolution(self,X, W, b):\n","      return np.sum(X * W) + b\n","\n","    def forward_convolution_step(self,data_in):\n","      m, n_H_prev, n_W_prev, n_C_prev = data_in.shape\n","\n","      n_H = int((n_H_prev - self.c_filter + 2 * self.c_pad) / self.c_stride) + 1\n","      n_W = int((n_W_prev - self.c_filter + 2 * self.c_pad) / self.c_stride) + 1\n","\n","      data_out = np.zeros((m, n_H, n_W, self.c_filters))\n","\n","      data_padded = self.add_pad(data_in,self.c_pad)\n","\n","      # Iteraciones\n","      for i in range(m):\n","          for h in range(n_H):\n","              vert_start = h * self.c_stride\n","              vert_end = vert_start + self.c_filter\n","              for w in range(n_W):\n","                  horiz_start = w * self.c_stride\n","                  horiz_end = horiz_start + self.c_filter\n","                  for c in range(self.c_filters):\n","                      data_slice = data_padded[i, vert_start:vert_end, horiz_start:horiz_end, :]\n","                      weights = self.W[..., c]\n","                      biases = self.b[..., c]\n","                      data_out[i, h, w, c] = self.one_convolution(data_slice,weights,biases)\n","\n","      return data_out\n","\n","    def forward_activation_step(self,data_in):\n","      activation_type = self.activation\n","      if activation_type == 'Relu':\n","          data_out = np.maximum(0, data_in)\n","      elif activation_type == 'Sigmoid':\n","          data_out = 1 / (1 + np.exp(-data_in))\n","      elif activation_type == 'Tanh':\n","          data_out = np.tanh(data_in)\n","      else:\n","          raise ValueError(\"Invalid activation type. Supported activations: 'Relu', 'Sigmoid', 'Tanh'.\")\n","\n","      return data_out\n","\n","    def forward_pooling_step(self,data_in):\n","      m, n_H_prev, n_W_prev, n_C_prev = data_in.shape\n","\n","      n_H = int((n_H_prev - self.p_filter) / self.p_stride) + 1\n","      n_W = int((n_W_prev - self.p_filter) / self.p_stride) + 1\n","\n","      data_out = np.zeros((m, n_H, n_W, n_C_prev))\n","\n","      # Iteraciones\n","      for i in range(m):\n","          for h in range(n_H):\n","              for w in range(n_W):\n","                  vert_start = h * self.p_stride\n","                  vert_end = vert_start + self.p_filter\n","                  horiz_start = w * self.p_stride\n","                  horiz_end = horiz_start + self.p_filter\n","                  data_slice = data_in[i, vert_start:vert_end, horiz_start:horiz_end, :]\n","\n","                  # Se aplica el pooling\n","                  if self.p_function == 'max':\n","                      data_out[i, h, w, :] = np.max(data_slice, axis=(0, 1))\n","                  elif self.p_function == 'average':\n","                      data_out[i, h, w, :] = np.mean(data_slice, axis=(0, 1))\n","                  else:\n","                      raise ValueError(\"Invalid pooling function. Supported functions: 'max', 'average'.\")\n","\n","      return data_out\n","\n","    def forward_convolutional(self, data_in):\n","      data_conv = self.forward_convolution_step(data_in)\n","      data_activation = self.forward_activation_step(data_conv)\n","      data_out = self.forward_pooling_step(data_activation)\n","      cache = (data_in, (data_conv, data_activation))\n","      return data_out, cache\n","\n","    def backward_pooling_step(self,gradient_in, cache):\n","      X = cache\n","      m, n_H_prev, n_W_prev, n_C_prev = X.shape\n","      m, n_H, n_W, n_C = gradient_in.shape\n","\n","      gradient_out = np.zeros(X.shape)\n","\n","      # Iteraciones\n","      for i in range(m):\n","          for h in range(n_H):\n","              for w in range(n_W):\n","                  vert_start = h\n","                  vert_end = vert_start + self.p_filter\n","                  horiz_start = w\n","                  horiz_end = horiz_start + self.p_filter\n","\n","                  if self.p_function == 'max':\n","                      data_slice = X[i, vert_start:vert_end, horiz_start:horiz_end, :]\n","                      mask = (data_slice == np.max(data_slice, axis=(0, 1)))\n","                      gradient_out[i, vert_start:vert_end, horiz_start:horiz_end, :] += mask * gradient_in[i, h, w, :]\n","                  elif self.p_function == 'avg':\n","                      avg_grad = gradient_in[i, h, w, :] / (self.p_filter ** 2)\n","                      gradient_out[i, vert_start:vert_end, horiz_start:horiz_end, :] += np.ones((self.p_filter, self.p_filter, n_C_prev)) * avg_grad\n","\n","      return gradient_out\n","\n","    def backward_activation_step(self,gradient_in, cache):\n","      activation_type = self.activation\n","      if not cache:\n","          if activation_type == 'Relu':\n","            grad_out = np.where(gradient_in > 0, 1, 0)  # Convertir valores positivos a 1 y valores negativos a 0\n","          elif activation_type == 'Sigmoid':\n","            grad_out = gradient_in * (gradient_in * (1 - cache[0]))\n","          elif activation_type == 'Tanh':\n","            grad_out = gradient_in * (1 - gradient_in**2)\n","          else:\n","            raise ValueError(\"Invalid activation type. Supported activations: 'Relu', 'Sigmoid', 'Tanh'.\")\n","      else:\n","          if activation_type == 'Relu':\n","              grad_out = np.where(cache[0] > 0, gradient_in, 0)  # Aplicar ReLU y redondear a 1 o 0\n","          elif activation_type == 'Sigmoid':\n","              grad_out = gradient_in * (cache[0] * (1 - cache))\n","          elif activation_type == 'Tanh':\n","              grad_out = gradient_in * (1 - cache[0]**2)\n","          else:\n","              raise ValueError(\"Invalid activation type. Supported activations: 'Relu', 'Sigmoid', 'Tanh'.\")\n","\n","      return grad_out\n","\n","    def backward_convolution_step(self,gradient_in, cache):\n","      # Desempaquetar cache\n","      (A_p) = cache\n","\n","      # Obtener dimensiones\n","      (m, n_H_prev, n_W_prev, n_C_prev) = A_p.shape\n","      (f, f, n_C_prev, n_C) = self.W.shape\n","\n","      # Retrieve dimensions from dZ's shape\n","      (m, n_H, n_W, n_C) = gradient_in.shape\n","\n","      # Initialize dA_prev, dW, db with the correct shapes\n","      dA_prev = np.zeros(A_p.shape)\n","      dW = np.zeros(self.W.shape)\n","      db = np.sum(gradient_in, axis=(0,1,2), keepdims=True)\n","\n","      # Pad A_prev and dA_prev\n","      data_in_pad = self.add_pad(A_p, self.c_pad)\n","      dA_prev_pad = self.add_pad(dA_prev, self.c_pad)\n","\n","      # Rellenar el gradiente dA_prev usando la fórmula de retropropagación\n","      for i in range(m):                       # loop sobre los ejemplos de entrenamiento\n","          a_prev_pad = data_in_pad[i]\n","          da_prev_pad = dA_prev_pad[i]\n","\n","          for h in range(n_H_prev):           # loop sobre la altura del volumen de salida\n","              for w in range(n_W_prev):       # loop sobre la anchura del volumen de salida\n","                  for c in range(n_C):        # loop sobre los canales del volumen de salida\n","                      # Encontrar las esquinas de la ventana de inicio\n","                      vert_start = h * self.c_stride\n","                      vert_end = vert_start + self.c_filter\n","                      horiz_start = w * self.c_stride\n","                      horiz_end = horiz_start + self.c_filter\n","\n","                      # Use the corners to define the slice from a_prev_pad\n","                      a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n","\n","                      # Update gradients for the window and the filter's parameters using the code formulas given above\n","                      da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += (self.W[..., c] * gradient_in[i, h, w, c])\n","                      dW[..., c] += a_slice * gradient_in[i, h, w, c]\n","                      #db[:,:,:,c] += gradient_in[i, h, w, c]\n","\n","          dA_prev[i, :, :, :] = da_prev_pad[self.c_pad:-self.c_pad, self.c_pad:-self.c_pad, :]\n","\n","      # Making sure your output shape is correct\n","      assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n","\n","      parameters = {'W':dW,'b':db}\n","\n","      #return dA_prev, parameters\n","      return dA_prev, parameters\n","\n","    def backward_convolutional(self,gradient_in,cache):\n","      (I,(A_p)) = cache\n","      # Retropropagación del paso de pooling\n","      gradient_out_pool = self.backward_pooling_step(gradient_in, (I))\n","      # Retropropagación del paso de activación\n","      gradient_out_act = self.backward_activation_step(gradient_out_pool, ())\n","      # Retropropagación del paso de convolución\n","      gradient_out_conv, parameters_gradient_conv = self.backward_convolution_step(gradient_out_act,(A_p))\n","\n","      return gradient_out_conv, parameters_gradient_conv"]},{"cell_type":"code","source":["##test: forward_convolutional\n","np.random.seed(1)\n","X = np.random.randn(2, 5, 7, 4)\n","s={'c_filter': 3,'c_channels': 4, 'c_filters': 8,'c_stride':2, 'c_pad':1, 'activation':'Relu', 'p_filter':2, 'p_stride' : 2, 'p_function': 'max'}\n","conv = MyConv(**s)\n","Y,_=conv.forward_convolutional(X)\n","Y[0,:,:,1]\n","assert Y.shape==(2,1,2,8), \"Wrong size\"\n","assert np.all(np.isclose(Y[0,:,:,1],[[4.2986541519, 3.4085979528]])), \"Wrong values\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WaUx5fOhn7a","executionInfo":{"status":"ok","timestamp":1713224078227,"user_tz":300,"elapsed":376,"user":{"displayName":"juan camargo","userId":"17479442830402474410"}},"outputId":"483a86c9-21e1-4d8c-c159-a570d32784f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-dc75d1790ec7>:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  data_out[i, h, w, c] = self.one_convolution(data_slice,weights,biases)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[4.29865415, 3.40859795]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["np.random.seed(1)\n","A_p = np.random.randn(10, 4, 4, 3)\n","s={'c_filter':2, 'c_pad' : 2,'c_stride': 2,'c_channels': 3, 'c_filters':8, 'activation':'Relu', 'p_stride' : 1, 'p_filter': 2, 'p_function':'max'}\n","conv = MyConv(**s)\n","#backward_convolutional\n","Z=conv.forward_convolution_step(A_p)\n","I=conv.forward_activation_step(Z )\n","A=conv.forward_pooling_step(I)\n","(m,h,w,c)=A.shape\n","dA=np.random.randn(m,h,w,c)\n","(dA_p, parameters_gradients)=conv.backward_convolutional(dA,(I,(A_p)))\n","dW=parameters_gradients['W']\n","db=parameters_gradients['b']\n","assert dA_p.shape == (10, 4, 4, 3), f\"Wrong shape for dA  {dA.shape} != (10, 4, 4, 3)\"\n","assert dW.shape == (2, 2, 3, 8), f\"Wrong shape for dW {dW.shape} != (2, 2, 3, 8)\"\n","assert db.shape == (1, 1, 1, 8), f\"Wrong shape for db {db.shape} != (1, 1, 1, 8)\"\n","np.mean(dA_p)\n","np.mean(dW)\n","np.mean(db)\n","assert np.isclose(np.mean(dA_p), 0.0743703), \"Wrong value for dA_p\"\n","assert np.isclose(np.mean(dW), 1.2200867), \"Wrong value for dW\"\n","assert np.isclose(np.mean(db), 47.125), \"Wrong value for db\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GRk_pVwVScR","executionInfo":{"status":"ok","timestamp":1713224080667,"user_tz":300,"elapsed":364,"user":{"displayName":"juan camargo","userId":"17479442830402474410"}},"outputId":"7291cd54-d7c3-4c44-a7ac-f32e5525a080"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-dc75d1790ec7>:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  data_out[i, h, w, c] = self.one_convolution(data_slice,weights,biases)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.07437035971362264"]},"metadata":{},"execution_count":4},{"output_type":"execute_result","data":{"text/plain":["1.2200867898708714"]},"metadata":{},"execution_count":4},{"output_type":"execute_result","data":{"text/plain":["47.125"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# **Densa**\n"],"metadata":{"id":"9kx988earVz9"}},{"cell_type":"code","source":["class MyDense:\n","    def __init__(self,layers,activations):\n","      self.layers=layers;\n","      self.red={}\n","      self.activations = activations\n","      self.initialize_parameters(layers)\n","\n","    def initialize_parameters(self,layers):\n","      for i in range(1, len(layers)):\n","          auxW = np.random.randn(layers[i], layers[i-1]) * 0.01\n","          auxB = np.zeros((layers[i], 1))\n","          self.red[\"W\" + str(i)] = auxW\n","          self.red[\"b\" + str(i)] = auxB\n","\n","    def forward_step(self,A_1, W, b, g):\n","      zeta = W @ A_1 + b\n","      zeta = zeta.astype(np.float128)\n","\n","      if g == \"Sigmoid\":\n","          A = 1 / (1 + np.exp(-zeta))\n","      elif g == \"Relu\":\n","          A = np.maximum(0, zeta)\n","      elif g == \"Softmax\":\n","          exp_x = np.exp(zeta - np.max(zeta))\n","          A = exp_x / np.sum(exp_x, axis=0)\n","      elif g == \"Tanh\":\n","          A = (np.exp(zeta) - np.exp(-zeta)) / (np.exp(zeta) + np.exp(-zeta))\n","      else:\n","          A = zeta\n","\n","      return A, np.array([A_1, W, A, g], dtype=object)\n","\n","    def forward(self,X, parameters):\n","      A_1 = X\n","      caches = []\n","\n","      for i in range(1, int(len(parameters) / 2) + 1):\n","          W = parameters[\"W\" + str(i)]\n","          b = parameters[\"b\" + str(i)]\n","\n","          A, cache = self.forward_step(A_1, W, b, self.activations[i - 1])\n","          caches.append(cache)\n","          A_1 = A\n","\n","      return A, caches\n","\n","    def predict(self,X, parameters):\n","      A = X\n","      caches = []\n","      L = len(parameters) // 2\n","\n","      for l in range(1, L + 1):\n","          A_prev = A\n","          W = parameters[f'W{l}']\n","          b = parameters[f'b{l}']\n","          A, cache = self.forward_step(A_prev, W, b, self.activations[l - 1])\n","          caches.append(cache)\n","\n","      return A, caches\n","\n","    def accuracy(self,X, Y):\n","      AL = self.predict(X, self.parameters, self.activations)\n","      print(\"Predictions:\", AL)\n","      predictions = np.argmax(AL, axis=0)\n","      print(\"Predicted labels:\", predictions)\n","      true_labels = np.argmax(Y, axis=0) #Puse esto para poder promediar cada etiqueta que va surgiendo para poder llegar a un valor aproximado\n","      print(\"True labels:\", true_labels)\n","      accuracy = np.mean(predictions == true_labels)\n","      print(\"Accuracy:\", accuracy)\n","      return accuracy\n","\n","    def cost(Yp, Y, cost):\n","      Yp = np.maximum(Yp, sys.float_info.min)\n","      m = Y.shape[1]\n","      if cost == \"Binary\":\n","          cost = -1/m * np.sum(Y * np.log(Yp + sys.float_info.epsilon) + (1 - Y) * np.log(1 - Yp + sys.float_info.epsilon))\n","      elif cost == \"Cathegory\":\n","          cost = -np.mean(np.sum(Y * np.log(Yp + sys.float_info.epsilon), axis=0))\n","      else:\n","          cost = np.mean(np.square(Yp - Y))\n","      return cost\n","\n","    def backward_step(self,dA, cache):\n","      A_prev, W, A, _ = cache\n","      m = A_prev.shape[1]\n","\n","      if self.activation == \"Sigmoid\":\n","          dZ = dA * A * (1 - A)\n","      elif self.activation == \"Relu\":\n","          dZ = np.array(dA, copy=True)\n","          dZ[A <= 0] = 0\n","      else:\n","          dZ = dA\n","\n","      dA_prev = np.dot(W.T, dZ)\n","      dW = (1 / m) * np.dot(dZ, A_prev.T)\n","      db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n","\n","      return dA_prev, dW, db\n","\n","    def backward(self,AL, Y, caches, cost_type):\n","      grads = {}\n","      L = len(caches)\n","      m = AL.shape[1]\n","      Y = Y.reshape(AL.shape)\n","\n","      if cost_type == \"Binary\":\n","          dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","      elif cost_type == \"Category\":\n","          dAL = - (np.divide(Y, AL))\n","      else:\n","          dAL = 2 * (AL - Y)\n","\n","      for l in reversed(range(L)):\n","          current_cache = caches[l]\n","          dA_prev_temp, dW_temp, db_temp = self.backward_step(dAL, current_cache, self.activations[l])\n","          grads[\"dW\" + str(l + 1)] = dW_temp\n","          grads[\"db\" + str(l + 1)] = db_temp\n","          dAL = dA_prev_temp\n","\n","      return grads\n","\n","\n","\n"],"metadata":{"id":"LVm7ZA3trZZB"},"execution_count":null,"outputs":[]}]}